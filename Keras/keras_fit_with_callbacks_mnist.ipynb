{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_fit_with_callbacks_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vRgMOgH-ajJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up Google Colab environment"
      ]
    },
    {
      "metadata": {
        "id": "7F29nuHfiiUA",
        "colab_type": "code",
        "outputId": "268d392e-d6a9-4e88-8f5a-eda1ea636444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "### Set colaboratory True to run in Google Colaboratory. \n",
        "colab = True\n",
        "\n",
        "if colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  import os\n",
        "  ## Specify a directory in Google Drive\n",
        "  dir = '/content/drive/My Drive/Colab Notebooks/Keras_MNIST'\n",
        "  os.chdir(dir)\n",
        "  #os.getcwd()\n",
        "  #os.listdir()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "As19XFC8jE-g",
        "colab_type": "code",
        "outputId": "688ac067-e914-4bd5-ff6d-b015e8e8f485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "if colab:\n",
        "  ## Check the uptime. (Google Colab reboots every 12 hours)\n",
        "  !cat /proc/uptime | awk '{print \"Uptime is \" $1 /60 /60 \" hours (\" $1 \" sec)\"}'\n",
        "  ## Check the GPU info\n",
        "  !nvidia-smi\n",
        "  ## Check the Python version\n",
        "  !python --version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uptime is 0.333539 hours (1200.74 sec)\n",
            "Fri Jan 25 13:37:21 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    55W / 149W |    599MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Python 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KN-GFrsvarFB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ]
    },
    {
      "metadata": {
        "id": "HZ219oTUjJXf",
        "colab_type": "code",
        "outputId": "a9912392-13be-4462-a96a-6c69115b856d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "MpoVWvOdxUgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up data"
      ]
    },
    {
      "metadata": {
        "id": "65zkKSbQ3Jnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_labels = train_labels[:1000]\n",
        "test_labels = test_labels[:1000]\n",
        "\n",
        "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
        "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciSTm1zha2ki",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ]
    },
    {
      "metadata": {
        "id": "k8pNftRxLT1i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Returns a short sequential model\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(10, activation=tf.keras.activations.softmax)\n",
        "  ])\n",
        "  \n",
        "  model.compile(optimizer='adam', \n",
        "                loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwFUcdCfa4Vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fit model"
      ]
    },
    {
      "metadata": {
        "id": "jrXvLv4kjx-v",
        "colab_type": "code",
        "outputId": "0529f9f8-e3b5-4631-80fb-73794196036b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1367
        }
      },
      "cell_type": "code",
      "source": [
        "### If resume_if_possible = True, fitting can resume using a model file. \n",
        "resume_if_possible = True \n",
        "\n",
        "### Set resume_if_possible = False to force fitting from scratch.\n",
        "#resume_if_possible = False \n",
        "\n",
        "epochs = 100\n",
        "log_file_path = 'training_log.csv'\n",
        "model_file_path = 'model_epoch{epoch:06d}.hdf5'\n",
        "\n",
        "model_file_list = glob.glob('model_epoch*.hdf5')\n",
        "log_file_list = glob.glob(log_file_path)\n",
        "\n",
        "if resume_if_possible:\n",
        "  resume_flag = (len(model_file_list) >= 1) \n",
        "  \n",
        "if not resume_if_possible:\n",
        "  resume_flag = False\n",
        "  for f in model_file_list:\n",
        "    os.remove(f)\n",
        "\n",
        "if resume_flag:\n",
        "  latest_model_file = model_file_list[-1]\n",
        "  latest_epoch = int(latest_model_file[len('model_epoch'):-len('.hdf5')])\n",
        "  ## Load the saved model\n",
        "  model = keras.models.load_model(latest_model_file)\n",
        "\n",
        "  score = model.evaluate(test_images,test_labels, verbose=0)\n",
        "  print('Use {} to resume fitting. \\nTest loss: {}   Test accuracy: {}'.format(latest_model_file, score[0], score[1]))\n",
        "\n",
        "  if len(model_file_list) >= 2:\n",
        "    ## Delete all model files excpet the latest to save space\n",
        "    for f in model_file_list[:-1]:\n",
        "      os.remove(f)\n",
        "\n",
        "if not resume_flag:\n",
        "  latest_epoch = 0\n",
        "  ## Create a basic model instance\n",
        "  model = create_model()\n",
        "  for f in log_file_list:\n",
        "    os.remove(f)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "## Create checkpoint callback\n",
        "check_point_ = tf.keras.callbacks.ModelCheckpoint(filepath = model_file_path, \n",
        "                                                 monitor = 'val_acc',\n",
        "                                                 verbose=1,\n",
        "                                                 save_best_only = True,\n",
        "                                                 mode='auto',\n",
        "                                                 save_weights_only=False,\n",
        "                                                 period = 1)\n",
        "\n",
        "## Create early stopping callback\n",
        "early_stopping_ = tf.keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "                                                   min_delta=0, \n",
        "                                                   patience=3, \n",
        "                                                   verbose=1, \n",
        "                                                   mode='auto', \n",
        "                                                   baseline=None)\n",
        "\n",
        "## Create CSV logger callback\n",
        "csv_logger_ = tf.keras.callbacks.CSVLogger(filename = log_file_path, separator=',',\n",
        "                                           append = resume_flag)\n",
        "\n",
        "## Fit \n",
        "model.fit(train_images, train_labels, epochs = epochs, initial_epoch = latest_epoch,\n",
        "          validation_data = (test_images,test_labels),\n",
        "          callbacks = [check_point_, early_stopping_, csv_logger_])\n",
        "\n",
        "## Remove duplicates as save_best_only option of checkpoint is set to True.\n",
        "pd.options.display.max_rows = 8\n",
        "log_df = pd.read_csv(log_file_path)\n",
        "log_wo_dup_df = log_df.drop_duplicates(subset = ['epoch'], keep='last').reset_index(drop=True)\n",
        "display(log_wo_dup_df)\n",
        "log_wo_dup_df.to_csv((log_file_path[:-len('.csv')] + '_processed.csv'), index=False)\n",
        "\n",
        "print('\\nFiles in the working directoy:')\n",
        "display(os.listdir())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            " 896/1000 [=========================>....] - ETA: 0s - loss: 1.2307 - acc: 0.6529\n",
            "Epoch 00001: val_acc improved from -inf to 0.77400, saving model to model_epoch000001.hdf5\n",
            "1000/1000 [==============================] - 1s 765us/step - loss: 1.1847 - acc: 0.6650 - val_loss: 0.7428 - val_acc: 0.7740\n",
            "Epoch 2/100\n",
            " 800/1000 [=======================>......] - ETA: 0s - loss: 0.4314 - acc: 0.8800\n",
            "Epoch 00002: val_acc improved from 0.77400 to 0.84500, saving model to model_epoch000002.hdf5\n",
            "1000/1000 [==============================] - 0s 228us/step - loss: 0.4201 - acc: 0.8820 - val_loss: 0.5141 - val_acc: 0.8450\n",
            "Epoch 3/100\n",
            " 896/1000 [=========================>....] - ETA: 0s - loss: 0.2938 - acc: 0.9252\n",
            "Epoch 00003: val_acc did not improve from 0.84500\n",
            "1000/1000 [==============================] - 0s 177us/step - loss: 0.2936 - acc: 0.9240 - val_loss: 0.4945 - val_acc: 0.8440\n",
            "Epoch 4/100\n",
            " 864/1000 [========================>.....] - ETA: 0s - loss: 0.2311 - acc: 0.9433\n",
            "Epoch 00004: val_acc improved from 0.84500 to 0.86100, saving model to model_epoch000004.hdf5\n",
            "1000/1000 [==============================] - 0s 220us/step - loss: 0.2252 - acc: 0.9460 - val_loss: 0.4350 - val_acc: 0.8610\n",
            "Epoch 5/100\n",
            " 864/1000 [========================>.....] - ETA: 0s - loss: 0.1580 - acc: 0.9711\n",
            "Epoch 00005: val_acc improved from 0.86100 to 0.86600, saving model to model_epoch000005.hdf5\n",
            "1000/1000 [==============================] - 0s 218us/step - loss: 0.1529 - acc: 0.9700 - val_loss: 0.4132 - val_acc: 0.8660\n",
            "Epoch 6/100\n",
            " 896/1000 [=========================>....] - ETA: 0s - loss: 0.1243 - acc: 0.9777\n",
            "Epoch 00006: val_acc improved from 0.86600 to 0.86900, saving model to model_epoch000006.hdf5\n",
            "1000/1000 [==============================] - 0s 218us/step - loss: 0.1191 - acc: 0.9790 - val_loss: 0.3973 - val_acc: 0.8690\n",
            "Epoch 7/100\n",
            " 800/1000 [=======================>......] - ETA: 0s - loss: 0.0910 - acc: 0.9838\n",
            "Epoch 00007: val_acc did not improve from 0.86900\n",
            "1000/1000 [==============================] - 0s 206us/step - loss: 0.0918 - acc: 0.9830 - val_loss: 0.4072 - val_acc: 0.8640\n",
            "Epoch 8/100\n",
            " 800/1000 [=======================>......] - ETA: 0s - loss: 0.0730 - acc: 0.9912\n",
            "Epoch 00008: val_acc did not improve from 0.86900\n",
            "1000/1000 [==============================] - 0s 193us/step - loss: 0.0719 - acc: 0.9910 - val_loss: 0.3989 - val_acc: 0.8670\n",
            "Epoch 9/100\n",
            " 832/1000 [=======================>......] - ETA: 0s - loss: 0.0483 - acc: 0.9976\n",
            "Epoch 00009: val_acc did not improve from 0.86900\n",
            "1000/1000 [==============================] - 0s 192us/step - loss: 0.0492 - acc: 0.9980 - val_loss: 0.4222 - val_acc: 0.8600\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>acc</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.665</td>\n",
              "      <td>1.184691</td>\n",
              "      <td>0.774</td>\n",
              "      <td>0.742757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.882</td>\n",
              "      <td>0.420127</td>\n",
              "      <td>0.845</td>\n",
              "      <td>0.514086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.293613</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.494548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.946</td>\n",
              "      <td>0.225225</td>\n",
              "      <td>0.861</td>\n",
              "      <td>0.434977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.979</td>\n",
              "      <td>0.119095</td>\n",
              "      <td>0.869</td>\n",
              "      <td>0.397312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.983</td>\n",
              "      <td>0.091834</td>\n",
              "      <td>0.864</td>\n",
              "      <td>0.407204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.991</td>\n",
              "      <td>0.071896</td>\n",
              "      <td>0.867</td>\n",
              "      <td>0.398858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.049192</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.422156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    epoch    acc      loss  val_acc  val_loss\n",
              "0       0  0.665  1.184691    0.774  0.742757\n",
              "1       1  0.882  0.420127    0.845  0.514086\n",
              "2       2  0.924  0.293613    0.844  0.494548\n",
              "3       3  0.946  0.225225    0.861  0.434977\n",
              "..    ...    ...       ...      ...       ...\n",
              "5       5  0.979  0.119095    0.869  0.397312\n",
              "6       6  0.983  0.091834    0.864  0.407204\n",
              "7       7  0.991  0.071896    0.867  0.398858\n",
              "8       8  0.998  0.049192    0.860  0.422156\n",
              "\n",
              "[9 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Files in the working directoy:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['keras_fit_with_callbacks_mnist.ipynb',\n",
              " 'training_log.csv',\n",
              " 'model_epoch000001.hdf5',\n",
              " 'model_epoch000002.hdf5',\n",
              " 'model_epoch000004.hdf5',\n",
              " 'model_epoch000005.hdf5',\n",
              " 'model_epoch000006.hdf5',\n",
              " 'training_log_processed.csv']"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "K8zre0uERDU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "https://keras.io/callbacks/\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint"
      ]
    }
  ]
}