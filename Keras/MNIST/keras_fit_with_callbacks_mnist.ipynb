{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_fit_with_callbacks_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vRgMOgH-ajJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up Google Colab environment"
      ]
    },
    {
      "metadata": {
        "id": "7F29nuHfiiUA",
        "colab_type": "code",
        "outputId": "b4eb24bd-b2be-4f8b-fc87-84e248d37838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "cell_type": "code",
      "source": [
        "### Set colaboratory True to run in Google Colaboratory. \n",
        "colab = True\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import os\n",
        "    ## Specify a directory in Google Drive\n",
        "    dir = '/content/drive/My Drive/Colab Notebooks/Keras_MNIST'\n",
        "    os.chdir(dir)\n",
        "    #os.getcwd()\n",
        "    #os.listdir()\n",
        "\n",
        "    ## Check the uptime. (Google Colab reboots every 12 hours)\n",
        "    !cat /proc/uptime | awk '{print \"Uptime is \" $1 /60 /60 \" hours (\" $1 \" sec)\"}'\n",
        "    ## Check the GPU info\n",
        "    !nvidia-smi\n",
        "    ## Check the Python version\n",
        "    !python --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Uptime is 0.0993778 hours (357.76 sec)\n",
            "Tue Jan 29 15:26:19 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Python 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KN-GFrsvarFB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ]
    },
    {
      "metadata": {
        "id": "HZ219oTUjJXf",
        "colab_type": "code",
        "outputId": "b88ffa08-6fd0-4169-afcf-3b0775524c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "QwFUcdCfa4Vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up resumable logging feature"
      ]
    },
    {
      "metadata": {
        "id": "E9G7gnSRShaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "class LogFileProcessing():\n",
        "    def __init__(self, resume_if_possible = True, log_file_path = 'training_log.csv', verbose = 1):\n",
        "        self.log_file_path = log_file_path\n",
        "        self.verbose = verbose\n",
        "        self.update()\n",
        "        if not resume_if_possible:\n",
        "            self.clean_up()\n",
        "            self.update()\n",
        "    def update(self):\n",
        "        log_file_path = self.log_file_path\n",
        "        log_file_list = glob.glob(log_file_path)\n",
        "        log_file_list.sort()\n",
        "        self.log_file_list = log_file_list\n",
        "        if self.verbose >= 2:\n",
        "            print('Log file:', log_file_list)\n",
        "        log_file_exists = len(log_file_list) >= 1\n",
        "        self.log_file_exists = log_file_exists\n",
        "    def truncate(self, epoch_from_model_file):\n",
        "        log_file_path = self.log_file_path\n",
        "        self.update()\n",
        "        ## Remove rows after the epoch of the latest saved model \n",
        "        ## (needed because save_best_only option of checkpoint is set to True.)\n",
        "        epoch_from_log_file = int(self.get_column_value('epoch'))\n",
        "        epoch_diff = epoch_from_log_file - epoch_from_model_file \n",
        "        with open(log_file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        if self.verbose >= 3:\n",
        "            print('Log file contents read:', lines)\n",
        "        lines_truncated = lines[:-(epoch_diff + 1)]\n",
        "        with open(log_file_path, 'w') as f:\n",
        "            f.writelines(lines_truncated)\n",
        "        if self.verbose >= 3:\n",
        "            print('New log file contents', lines_truncated)\n",
        "    def clean_up(self):\n",
        "        self.update()\n",
        "        if self.log_file_exists:\n",
        "            for f in self.log_file_list:\n",
        "                os.remove(f)\n",
        "            self.update()\n",
        "    def get_df(self, dtype=None):\n",
        "        self.update()\n",
        "        if self.log_file_exists:\n",
        "            try:\n",
        "                return pd.read_csv(self.log_file_path, dtype=dtype)\n",
        "            except:\n",
        "                pass\n",
        "        if self.verbose >= 2:\n",
        "            print('Log file was not read as a DataFrame.')\n",
        "        return None\n",
        "    def get_column_value(self, column, default = '0'):\n",
        "        log_df = self.get_df(dtype=str)\n",
        "        if log_df is not None:\n",
        "            if log_df.shape[0] >= 1:\n",
        "                column_value = log_df.iloc[-1][column]\n",
        "                return column_value\n",
        "        if self.verbose >= 2:\n",
        "            print('{} assumed to be {}.'.format(column, default))\n",
        "        column_value = default\n",
        "        return column_value    \n",
        "    def plot_log(self, show_table = True):\n",
        "        log_all_df = self.get_df()\n",
        "        if log_all_df is not None:\n",
        "            model_id_list = log_all_df['_Model_id'].unique()\n",
        "            print('Model id list: ', model_id_list)\n",
        "            for model_id in model_id_list: \n",
        "                print('### Model id: ', model_id)\n",
        "                log_df = log_all_df.query('_Model_id == \"{}\"'.format(model_id))\n",
        "                if show_table:\n",
        "                    display(log_df)\n",
        "                log_df.set_index('epoch')[['acc', 'val_acc']].plot(style='.-', title='Accuracy vs Epoch')\n",
        "                plt.show()\n",
        "                log_df.set_index('epoch')[['loss', 'val_loss']].plot(style='.-', title='Loss vs Epoch')\n",
        "                plt.show()\n",
        "                log_df.set_index('epoch')[['Comp_time', 'Cum_comp_time']].plot(style='.-', title='Computation Time (sec) vs Epoch')\n",
        "                plt.show()          \n",
        "            \n",
        "class ModelFileProcessing():\n",
        "    def __init__(self, model_id = '', model_file_prefix = 'model', model_file_suffix = '.hdf5', verbose = 1):\n",
        "        model_file_prefix = model_file_prefix + model_id + '_epoch'\n",
        "        self.model_file_prefix = model_file_prefix\n",
        "        self.model_file_suffix = model_file_suffix\n",
        "        self.verbose = verbose\n",
        "        model_file_path = model_file_prefix + '{epoch:06d}' + model_file_suffix\n",
        "        self.model_file_path = model_file_path\n",
        "        self.update()\n",
        "    def update(self):\n",
        "        model_file_prefix = self.model_file_prefix\n",
        "        model_file_suffix = self.model_file_suffix\n",
        "        model_file_list = glob.glob(model_file_prefix + '*' + model_file_suffix)\n",
        "        model_file_list.sort()\n",
        "        if self.verbose >= 2:\n",
        "            print('Model files: ', model_file_list)\n",
        "        self.model_file_list = model_file_list\n",
        "        model_file_exists = len(model_file_list) >= 1\n",
        "        self.model_file_exists = model_file_exists\n",
        "\n",
        "        latest_model_file = model_file_list[-1] if model_file_exists else None\n",
        "        self.latest_model_file = latest_model_file\n",
        "\n",
        "        latest_epoch = int(latest_model_file[len(model_file_prefix):-len(model_file_suffix)]) \\\n",
        "            if model_file_exists else 0\n",
        "        self.latest_epoch = latest_epoch\n",
        "\n",
        "        multiple_model_files_exist = len(model_file_list) >= 2\n",
        "        self.multiple_model_files_exist = multiple_model_files_exist\n",
        "    def clean_up_old(self):\n",
        "        self.update()\n",
        "        ## Delete all model files excpet the latest to save space\n",
        "        if self.multiple_model_files_exist:\n",
        "            for f in self.model_file_list[:-1]:\n",
        "                os.remove(f)  \n",
        "    def clean_up_all(self):\n",
        "        self.update()\n",
        "        ## Delete all model files if they exist.\n",
        "        if self.model_file_exists:\n",
        "            for f in self.model_file_list:\n",
        "                os.remove(f)   \n",
        "                \n",
        "class ModelParams(keras.callbacks.Callback):\n",
        "    def __init__(self, model_param_dict = {}, verbose = 2):\n",
        "        self.model_param_dict = model_param_dict\n",
        "        self.verbose = verbose\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.verbose >= 3:\n",
        "            print('Model params: ', self.model_param_dict)\n",
        "        ## Add the model parameters to the logs\n",
        "        logs.update(self.model_param_dict)\n",
        "        if self.verbose >= 3:\n",
        "            print('logs: ', logs)\n",
        "        \n",
        "class OldModelFileCleanup(keras.callbacks.Callback):\n",
        "    def __init__(self, mfp):\n",
        "        self.mfp = mfp\n",
        "    def on_epoch_end(self, epoch, logs={}):     \n",
        "        self.mfp.clean_up_old()\n",
        "        \n",
        "import time          \n",
        "class CompTime(keras.callbacks.Callback):\n",
        "    def __init__(self, lfp):\n",
        "        self.lfp = lfp\n",
        "        self.verbose = lfp.verbose\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        ## Calculate computation time for the epoch\n",
        "        comp_time = time.time() - self.epoch_time_start\n",
        "        logs['Comp_time'] = comp_time\n",
        "        if self.verbose >= 2:\n",
        "            print('Computation time: {} sec'.format(comp_time))\n",
        "        ## Calculate cumulative computation time up to the epoch\n",
        "        cum_comp_time_past = \\\n",
        "        float(self.lfp.get_column_value('Cum_comp_time')) if epoch >= 1 else 0\n",
        "        logs['Cum_comp_time'] = cum_comp_time_past + comp_time\n",
        "          \n",
        "def setup_model_and_callbacks(lfp, create_model, input_shape, resume_if_possible = True,\n",
        "                             model_param_dict = {}, verbose = 1, early_stopping_patience = 5):\n",
        "    \n",
        "    mfp = ModelFileProcessing(model_id = model_param_dict['_Model_id'], verbose = 1)\n",
        "    resume_flag = resume_if_possible and mfp.model_file_exists and lfp.log_file_exists ## To Do: if the model Id entry exists in the CSV log file.\n",
        "\n",
        "    if resume_flag:\n",
        "        ## Load the saved model\n",
        "        model = keras.models.load_model(mfp.latest_model_file)\n",
        "        print('Use {} to resume fitting.'.format(mfp.latest_model_file))\n",
        "        initial_epoch = mfp.latest_epoch\n",
        "        ## Truncate the log file\n",
        "        lfp.truncate(epoch_from_model_file = mfp.latest_epoch) \n",
        "\n",
        "    if not resume_flag:\n",
        "        ## Create a basic model instance\n",
        "        model = create_model(input_shape)\n",
        "        print('Model created.')\n",
        "        initial_epoch = 0\n",
        "        ## Delete all model files if they exist.\n",
        "        mfp.clean_up_all()\n",
        "\n",
        "    ## Create checkpoint callback\n",
        "    check_point_ = tf.keras.callbacks.ModelCheckpoint(filepath = mfp.model_file_path, \n",
        "        monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'auto', \n",
        "        save_weights_only = False, period = 1)\n",
        "    \n",
        "    ## Create old model file cleanup callback\n",
        "    old_model_file_cleanup_ = OldModelFileCleanup(mfp)\n",
        "    \n",
        "    ## Create computation time callback\n",
        "    comp_time_ = CompTime(lfp)\n",
        "    \n",
        "    ## Create model params callback\n",
        "    model_params_ = ModelParams(model_param_dict)\n",
        "\n",
        "    ## Create CSV logger callback\n",
        "    csv_logger_ = tf.keras.callbacks.CSVLogger(filename = lfp.log_file_path, separator=',',\n",
        "                                               append = True)\n",
        "\n",
        "    ## Create early stopping callback\n",
        "    early_stopping_ = tf.keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "        min_delta=0, patience=early_stopping_patience, verbose=1, mode='auto', baseline=None)\n",
        "\n",
        "    callbacks = [check_point_, old_model_file_cleanup_, \n",
        "                 model_params_, comp_time_, csv_logger_, early_stopping_]\n",
        "\n",
        "    return model, callbacks, initial_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWwcu32T3aAp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up dataset"
      ]
    },
    {
      "metadata": {
        "id": "hp7CMqwQ3Ymp",
        "colab_type": "code",
        "outputId": "74f1e759-b7dd-4cc4-efb3-abf571d05a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "dataset_name = 'MNIST' ## logged in the log file.\n",
        "\n",
        "if dataset_name in ['MNIST', 'MNIST_1000samples']:\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    print('x_train: ', x_train.shape)\n",
        "    print('y_train', y_train.shape)\n",
        "    print('x_test: ', x_test.shape)\n",
        "    print('y_test', y_test.shape)\n",
        "\n",
        "    input_shape = x_train.shape[1:]\n",
        "    print('input_shape: ', input_shape )\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "    \n",
        "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "\n",
        "if dataset_name == 'MNIST_1000samples':\n",
        "    ## Reduce samples size to speed up \n",
        "    n = 1000\n",
        "    y_train = y_train[:n]\n",
        "    y_test = y_test[:n]\n",
        "    x_train = x_train[:n]\n",
        "    x_test = x_test[:n]\n",
        "    \n",
        "print('\\n### data after processing')\n",
        "print('x_train: ', x_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('x_test: ', x_test.shape)\n",
        "print('y_test', y_test.shape)\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "print('input_shape: ', input_shape )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train:  (60000, 28, 28)\n",
            "y_train (60000,)\n",
            "x_test:  (10000, 28, 28)\n",
            "y_test (10000,)\n",
            "input_shape:  (28, 28)\n",
            "\n",
            "### data after processing\n",
            "x_train:  (60000, 28, 28, 1)\n",
            "y_train (60000,)\n",
            "x_test:  (10000, 28, 28, 1)\n",
            "y_test (10000,)\n",
            "input_shape:  (28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "btAFN4H54FOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up model parameters"
      ]
    },
    {
      "metadata": {
        "id": "kgsQhkdy4DxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NotUsed=\"\"\"\n",
        "### Grid Search \n",
        " \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout\n",
        "from keras.optimizers import SGD, Adagrad, RMSprop, Adam\n",
        "\n",
        "batch_size_list = [64]\n",
        "\n",
        "optimizer_dict = {}\n",
        "#optimizer_dict['SGD'] = SGD()\n",
        "#optimizer_dict['SGD_momentum_0.5'] = SGD(momentum = 0.5)\n",
        "#optimizer_dict['SGD_momentum_0.9'] = SGD(momentum = 0.9)\n",
        "#optimizer_dict['SGD_momentum_0.99'] = SGD(momentum = 0.99)         \n",
        "#optimizer_dict['Adagrad'] = Adagrad()\n",
        "#optimizer_dict['RMSprop'] = RMSprop()\n",
        "optimizer_dict['Adam'] = Adam()\n",
        "\n",
        "activation_list = []\n",
        "activation_list.append('relu')\n",
        "#activation_list.append('sigmoid')\n",
        "#activation_list.append('elu')\n",
        "\n",
        "objective_list = []\n",
        "objective_list.append('sparse_categorical_crossentropy')\n",
        "#objective_list.append('categorical_crossentropy')\n",
        "\n",
        "dropout_rate_list = [0.0, 0.5]\n",
        "\n",
        "model_param_dict_list = []\n",
        "i = 0\n",
        "for batch_size in batch_size_list:\n",
        "    for optimizer, _ in optimizer_dict.items():\n",
        "        for activation in activation_list:\n",
        "            for objective in objective_list:\n",
        "                for dropout_rate in dropout_rate_list:\n",
        "                    d = {}\n",
        "                    d['_Dataset_name'] = dataset_name\n",
        "                    d['_Model_setup'] = 'Dense_Only'\n",
        "                    d['_Model_id'] = '{:06d}'.format(i)\n",
        "                    d['Batch_size'] = batch_size\n",
        "                    d['Optimizer'] = optimizer\n",
        "                    d['Acivation'] = activation\n",
        "                    d['Objective'] = objective\n",
        "                    d['Dropout_rate'] = dropout_rate\n",
        "                    i += 1\n",
        "                    print(d)\n",
        "                    model_param_dict_list.append(d)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZqBQ5UsCF12F",
        "colab_type": "code",
        "outputId": "4cf40d67-f8e4-4185-adc8-9f9e8d04e7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "cell_type": "code",
      "source": [
        "### derivative style \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D\n",
        "from keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "\n",
        "batch_size_list = [64]\n",
        "\n",
        "optimizer_dict = {}\n",
        "optimizer_dict['SGD'] = SGD()\n",
        "optimizer_dict['SGD_momentum_0.5'] = SGD(momentum = 0.5)\n",
        "optimizer_dict['SGD_momentum_0.9'] = SGD(momentum = 0.9)\n",
        "optimizer_dict['SGD_momentum_0.99'] = SGD(momentum = 0.99)         \n",
        "optimizer_dict['Adagrad'] = Adagrad()\n",
        "optimizer_dict['RMSprop'] = RMSprop()\n",
        "optimizer_dict['Adadelta'] = Adadelta()\n",
        "optimizer_dict['Adamax'] = Adamax()\n",
        "optimizer_dict['Adadelta'] = Adadelta()\n",
        "\n",
        "Nadam\n",
        "\n",
        "optimizer_dict['Adam'] = Adam(lr=0.001)\n",
        "optimizer_dict['Adam_lr0.0001'] = Adam(lr=0.0001)\n",
        "optimizer_dict['Adam_lr0.01'] = Adam(lr=0.01)\n",
        "optimizer_dict['Adam_lr0.1'] = Adam(lr=0.1)\n",
        "\n",
        "objective_list = []\n",
        "objective_list.append('sparse_categorical_crossentropy')\n",
        "#objective_list.append('categorical_crossentropy')\n",
        "\n",
        "dropout_rate_list = [0.0, 0.5]\n",
        "\n",
        "base_model_param_dict = \\\n",
        "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000000', \n",
        " 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', \n",
        " 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
        "derivative_dict_list = \\\n",
        "[\\\n",
        " {'Dropout_rate': 0.3},\n",
        " {'Dropout_rate': 0.5},\n",
        " {'Dropout_rate': 0.7},\n",
        " {'Dropout_rate': 0.9},\n",
        " {'Acivation': 'sigmoid'}, \n",
        " {'Acivation': 'elu'},\n",
        " {'Acivation': 'tanh'},\n",
        " {'Acivation': 'selu'},\n",
        " {'Acivation': 'hard_sigmoid'},\n",
        " {'Acivation': 'linear'},\n",
        " {'Optimizer': 'Adam_lr0.0001'},\n",
        " {'Optimizer': 'Adam_lr0.01'},\n",
        " {'Optimizer': 'Adam_lr0.1'},\n",
        " {'Optimizer': 'SGD'},\n",
        " {'Optimizer': 'SGD_momentum_0.5'},\n",
        " {'Optimizer': 'SGD_momentum_0.9'},\n",
        " {'Optimizer': 'SGD_momentum_0.99'},\n",
        " {'Optimizer': 'RMSprop'},\n",
        " {'Optimizer': 'Adagrad'},\n",
        " {'Optimizer': 'Adadelta'},\n",
        " {'Batch_size': 32},\n",
        " {'Batch_size': 128},\n",
        " {'Batch_size': 256}]\n",
        "\n",
        "model_param_dict_list = [base_model_param_dict]\n",
        "print(base_model_param_dict)\n",
        "\n",
        "for i, derivative_dict in enumerate(derivative_dict_list, 1):\n",
        "    model_param_dict = base_model_param_dict.copy()\n",
        "    model_param_dict.update({'_Model_id': '{:06d}'.format(i)})\n",
        "    model_param_dict.update(derivative_dict)\n",
        "    model_param_dict_list.append(model_param_dict)\n",
        "    print(model_param_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000000', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000001', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.3}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000002', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.5}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000003', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.7}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000004', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.9}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000005', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'sigmoid', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000006', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'elu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000007', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'tanh', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000008', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'selu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000009', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'hard_sigmoid', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000010', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'linear', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000011', 'Batch_size': 64, 'Optimizer': 'Adam_lr0.0001', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000012', 'Batch_size': 64, 'Optimizer': 'Adam_lr0.01', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000013', 'Batch_size': 64, 'Optimizer': 'Adam_lr0.1', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000014', 'Batch_size': 64, 'Optimizer': 'SGD', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000015', 'Batch_size': 64, 'Optimizer': 'SGD_momentum_0.5', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000016', 'Batch_size': 64, 'Optimizer': 'SGD_momentum_0.9', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000017', 'Batch_size': 64, 'Optimizer': 'SGD_momentum_0.99', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000018', 'Batch_size': 64, 'Optimizer': 'RMSprop', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000019', 'Batch_size': 64, 'Optimizer': 'Adagrad', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000020', 'Batch_size': 64, 'Optimizer': 'Adadelta', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000021', 'Batch_size': 32, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000022', 'Batch_size': 128, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000023', 'Batch_size': 256, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J-0RAJHl4i5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train models"
      ]
    },
    {
      "metadata": {
        "id": "jrXvLv4kjx-v",
        "colab_type": "code",
        "outputId": "8e3e9b2e-2c21-4bcf-d5a9-73b8cb1760fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11645
        }
      },
      "cell_type": "code",
      "source": [
        "## Set resume_if_possible = True to resume using a model file if available.\n",
        "## Set resume_if_possible = False to force fitting from scratch.\n",
        "#resume_if_possible = True\n",
        "resume_if_possible = False\n",
        "\n",
        "epochs = 10\n",
        "early_stopping_patience = 10\n",
        "\n",
        "lfp = LogFileProcessing(resume_if_possible = resume_if_possible, verbose = 1)\n",
        "\n",
        "initial_model_id_num = int(lfp.get_column_value('_Model_id'))\n",
        "\n",
        "for model_param_dict in model_param_dict_list[initial_model_id_num:]:\n",
        "    print(model_param_dict)\n",
        "    ## Assign variables as in the model_param_dict dictionary\n",
        "    globals().update(model_param_dict) \n",
        "    \n",
        "    if _Model_setup == 'Dense_Only':\n",
        "        def create_model(input_shape):\n",
        "            model = Sequential()\n",
        "            model.add(Flatten(input_shape = input_shape))\n",
        "            model.add(Dense(512, activation = Acivation))\n",
        "            model.add(Dropout(Dropout_rate))\n",
        "            model.add(Dense(10, activation='softmax'))\n",
        "            model.compile(optimizer = optimizer_dict[Optimizer], \n",
        "                          loss=Objective, metrics=['accuracy'])\n",
        "            return model\n",
        "    if _Model_setup == 'CNN': \n",
        "        def create_model(input_shape):\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(512, kernel_size = (3, 3), strides = (1, 1),\n",
        "                           activation = Acivation, input_shape = input_shape))\n",
        "            model.add(Flatten())\n",
        "            model.add(Dropout(Dropout_rate))\n",
        "            model.add(Dense(10, activation='softmax'))\n",
        "            model.compile(optimizer = optimizer_dict[Optimizer], \n",
        "                        loss=Objective, metrics=['accuracy'])\n",
        "            return model\n",
        "  \n",
        "    model, callbacks, initial_epoch = \\\n",
        "    setup_model_and_callbacks(lfp, create_model, input_shape, \n",
        "                              resume_if_possible = resume_if_possible,\n",
        "                              model_param_dict = model_param_dict, verbose = 1, \n",
        "                              early_stopping_patience = early_stopping_patience)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    ## Fit \n",
        "    model.fit(x_train, y_train, validation_data = (x_test, y_test), shuffle = True,\n",
        "              batch_size = Batch_size, epochs = epochs, callbacks = callbacks, \n",
        "              initial_epoch = initial_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000000', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 34s 569us/step - loss: 0.1389 - acc: 0.9593 - val_loss: 0.0697 - val_acc: 0.9776\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97760, saving model to model000000_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0528 - acc: 0.9843 - val_loss: 0.0595 - val_acc: 0.9813\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97760 to 0.98130, saving model to model000000_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0330 - acc: 0.9898 - val_loss: 0.0645 - val_acc: 0.9799\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98130\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0648 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98130 to 0.98170, saving model to model000000_epoch000004.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0794 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98170\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0125 - acc: 0.9955 - val_loss: 0.0745 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98170 to 0.98230, saving model to model000000_epoch000006.hdf5\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0080 - acc: 0.9973 - val_loss: 0.0774 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98230\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 30s 503us/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.1153 - val_acc: 0.9765\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98230\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0842 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.98230 to 0.98270, saving model to model000000_epoch000009.hdf5\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 30s 501us/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1057 - val_acc: 0.9786\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98270\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000001', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.3}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 42s 695us/step - loss: 0.1243 - acc: 0.9626 - val_loss: 0.0619 - val_acc: 0.9791\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97910, saving model to model000001_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0478 - acc: 0.9854 - val_loss: 0.0572 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97910 to 0.98240, saving model to model000001_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 42s 693us/step - loss: 0.0312 - acc: 0.9905 - val_loss: 0.0551 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98240\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 41s 689us/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.0649 - val_acc: 0.9815\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98240\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 41s 688us/step - loss: 0.0148 - acc: 0.9957 - val_loss: 0.0698 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98240 to 0.98250, saving model to model000001_epoch000005.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0729 - val_acc: 0.9818\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98250\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 41s 688us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0720 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98250 to 0.98260, saving model to model000001_epoch000007.hdf5\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 41s 688us/step - loss: 0.0070 - acc: 0.9974 - val_loss: 0.0794 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98260\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 41s 689us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0892 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98260\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 41s 689us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0909 - val_acc: 0.9807\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98260\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000002', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.5}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 42s 699us/step - loss: 0.1350 - acc: 0.9597 - val_loss: 0.0656 - val_acc: 0.9779\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97790, saving model to model000002_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 41s 689us/step - loss: 0.0536 - acc: 0.9837 - val_loss: 0.0581 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97790 to 0.98300, saving model to model000002_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 42s 704us/step - loss: 0.0390 - acc: 0.9881 - val_loss: 0.0623 - val_acc: 0.9808\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.98300\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0294 - acc: 0.9908 - val_loss: 0.0589 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98300\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 42s 698us/step - loss: 0.0245 - acc: 0.9920 - val_loss: 0.0629 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98300\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0186 - acc: 0.9939 - val_loss: 0.0639 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98300\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 41s 685us/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0733 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98300\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0729 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98300\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 44s 734us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0750 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98300\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 42s 699us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0824 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98300\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000003', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.7}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 42s 705us/step - loss: 0.1429 - acc: 0.9578 - val_loss: 0.0653 - val_acc: 0.9792\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97920, saving model to model000003_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 42s 699us/step - loss: 0.0671 - acc: 0.9796 - val_loss: 0.0624 - val_acc: 0.9800\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97920 to 0.98000, saving model to model000003_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 42s 696us/step - loss: 0.0534 - acc: 0.9835 - val_loss: 0.0593 - val_acc: 0.9814\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98000 to 0.98140, saving model to model000003_epoch000003.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 42s 696us/step - loss: 0.0449 - acc: 0.9858 - val_loss: 0.0580 - val_acc: 0.9809\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98140\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 42s 695us/step - loss: 0.0387 - acc: 0.9877 - val_loss: 0.0587 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98140 to 0.98220, saving model to model000003_epoch000005.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 42s 695us/step - loss: 0.0332 - acc: 0.9895 - val_loss: 0.0561 - val_acc: 0.9843\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98220 to 0.98430, saving model to model000003_epoch000006.hdf5\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 42s 694us/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0599 - val_acc: 0.9834\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98430\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0262 - acc: 0.9915 - val_loss: 0.0611 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98430\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0258 - acc: 0.9912 - val_loss: 0.0639 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98430\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 41s 691us/step - loss: 0.0233 - acc: 0.9922 - val_loss: 0.0619 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.98430 to 0.98450, saving model to model000003_epoch000010.hdf5\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000004', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.9}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 42s 701us/step - loss: 0.2307 - acc: 0.9295 - val_loss: 0.0904 - val_acc: 0.9720\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.97200, saving model to model000004_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 41s 691us/step - loss: 0.1312 - acc: 0.9602 - val_loss: 0.0714 - val_acc: 0.9780\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.97200 to 0.97800, saving model to model000004_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.1113 - acc: 0.9660 - val_loss: 0.0638 - val_acc: 0.9782\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97800 to 0.97820, saving model to model000004_epoch000003.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.1003 - acc: 0.9692 - val_loss: 0.0643 - val_acc: 0.9797\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97820 to 0.97970, saving model to model000004_epoch000004.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 41s 691us/step - loss: 0.0913 - acc: 0.9718 - val_loss: 0.0573 - val_acc: 0.9805\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.97970 to 0.98050, saving model to model000004_epoch000005.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0854 - acc: 0.9727 - val_loss: 0.0572 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98050 to 0.98170, saving model to model000004_epoch000006.hdf5\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 41s 692us/step - loss: 0.0819 - acc: 0.9739 - val_loss: 0.0571 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98170\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 41s 691us/step - loss: 0.0764 - acc: 0.9755 - val_loss: 0.0541 - val_acc: 0.9834\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.98170 to 0.98340, saving model to model000004_epoch000008.hdf5\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 41s 690us/step - loss: 0.0751 - acc: 0.9759 - val_loss: 0.0526 - val_acc: 0.9829\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98340\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 41s 691us/step - loss: 0.0717 - acc: 0.9762 - val_loss: 0.0549 - val_acc: 0.9826\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98340\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000005', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'sigmoid', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 31s 513us/step - loss: 14.5073 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.09580, saving model to model000005_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 30s 503us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.09580\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.09580\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 30s 503us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.09580\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.09580\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.09580\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 30s 503us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.09580\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 30s 508us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.09580\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 31s 508us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.09580\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.09580\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000006', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'elu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 32s 532us/step - loss: 0.4502 - acc: 0.8899 - val_loss: 0.2961 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91970, saving model to model000006_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 0.3164 - acc: 0.9135 - val_loss: 0.3215 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.91970\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 31s 521us/step - loss: 0.3051 - acc: 0.9162 - val_loss: 0.3660 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91970\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.3077 - acc: 0.9150 - val_loss: 0.3211 - val_acc: 0.9116\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91970\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 31s 522us/step - loss: 0.2927 - acc: 0.9188 - val_loss: 0.3075 - val_acc: 0.9242\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.91970 to 0.92420, saving model to model000006_epoch000005.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.2848 - acc: 0.9211 - val_loss: 0.3550 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.92420\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.2755 - acc: 0.9246 - val_loss: 0.2843 - val_acc: 0.9213\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.92420\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.2629 - acc: 0.9261 - val_loss: 0.3063 - val_acc: 0.9166\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.92420\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 31s 523us/step - loss: 0.2531 - acc: 0.9303 - val_loss: 0.2971 - val_acc: 0.9245\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.92420 to 0.92450, saving model to model000006_epoch000009.hdf5\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 31s 524us/step - loss: 0.2453 - acc: 0.9315 - val_loss: 0.2794 - val_acc: 0.9287\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.92450 to 0.92870, saving model to model000006_epoch000010.hdf5\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000007', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'tanh', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 31s 519us/step - loss: 0.4959 - acc: 0.8867 - val_loss: 0.3279 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91990, saving model to model000007_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.3125 - acc: 0.9141 - val_loss: 0.3274 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.91990\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.3083 - acc: 0.9153 - val_loss: 0.3004 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.91990\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.3021 - acc: 0.9162 - val_loss: 0.3096 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.91990\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.3028 - acc: 0.9161 - val_loss: 0.3210 - val_acc: 0.9170\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91990\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.2947 - acc: 0.9177 - val_loss: 0.3098 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.91990\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.2943 - acc: 0.9192 - val_loss: 0.3315 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91990\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 31s 510us/step - loss: 0.2887 - acc: 0.9202 - val_loss: 0.2997 - val_acc: 0.9224\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91990 to 0.92240, saving model to model000007_epoch000008.hdf5\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.2823 - acc: 0.9217 - val_loss: 0.3208 - val_acc: 0.9142\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.92240\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 31s 509us/step - loss: 0.2729 - acc: 0.9249 - val_loss: 0.3454 - val_acc: 0.9128\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.92240\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000008', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'selu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 49s 819us/step - loss: 1.0091 - acc: 0.8797 - val_loss: 0.3088 - val_acc: 0.9128\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91280, saving model to model000008_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 48s 806us/step - loss: 0.2894 - acc: 0.9231 - val_loss: 0.2619 - val_acc: 0.9306\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.91280 to 0.93060, saving model to model000008_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.2529 - acc: 0.9311 - val_loss: 0.2312 - val_acc: 0.9342\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.93060 to 0.93420, saving model to model000008_epoch000003.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.2338 - acc: 0.9363 - val_loss: 0.2158 - val_acc: 0.9370\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.93420 to 0.93700, saving model to model000008_epoch000004.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 48s 798us/step - loss: 0.2164 - acc: 0.9417 - val_loss: 0.2527 - val_acc: 0.9310\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.93700\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 48s 797us/step - loss: 0.2094 - acc: 0.9448 - val_loss: 0.2766 - val_acc: 0.9314\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.93700\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 48s 796us/step - loss: 0.2018 - acc: 0.9468 - val_loss: 0.2327 - val_acc: 0.9469\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.93700 to 0.94690, saving model to model000008_epoch000007.hdf5\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 48s 796us/step - loss: 0.1933 - acc: 0.9496 - val_loss: 0.2159 - val_acc: 0.9527\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.94690 to 0.95270, saving model to model000008_epoch000008.hdf5\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 48s 795us/step - loss: 0.1759 - acc: 0.9536 - val_loss: 0.2533 - val_acc: 0.9467\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.95270\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 48s 795us/step - loss: 0.1788 - acc: 0.9536 - val_loss: 0.2215 - val_acc: 0.9493\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.95270\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000009', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'hard_sigmoid', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 50s 838us/step - loss: 14.5081 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.09580, saving model to model000009_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 47s 789us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.09580\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 47s 788us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.09580\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 48s 794us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.09580\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 48s 796us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.09580\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 48s 795us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.09580\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 48s 795us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.09580\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 48s 793us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.09580\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 48s 794us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.09580\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 48s 795us/step - loss: 14.5283 - acc: 0.0986 - val_loss: 14.5740 - val_acc: 0.0958\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.09580\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000010', 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'linear', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 1.1680 - acc: 0.8613 - val_loss: 0.3121 - val_acc: 0.9145\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.91450, saving model to model000010_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.3197 - acc: 0.9132 - val_loss: 0.3229 - val_acc: 0.9092\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.91450\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.3072 - acc: 0.9153 - val_loss: 0.3089 - val_acc: 0.9163\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.91450 to 0.91630, saving model to model000010_epoch000003.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.3111 - acc: 0.9142 - val_loss: 0.3092 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.91630 to 0.91650, saving model to model000010_epoch000004.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.2996 - acc: 0.9170 - val_loss: 0.3353 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.91650\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 27s 454us/step - loss: 0.3013 - acc: 0.9166 - val_loss: 0.3045 - val_acc: 0.9153\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.91650\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.3000 - acc: 0.9175 - val_loss: 0.3603 - val_acc: 0.9103\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91650\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.2927 - acc: 0.9190 - val_loss: 0.3560 - val_acc: 0.9072\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.91650\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 27s 453us/step - loss: 0.2936 - acc: 0.9183 - val_loss: 0.3793 - val_acc: 0.9064\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91650\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 27s 455us/step - loss: 0.2891 - acc: 0.9203 - val_loss: 0.3129 - val_acc: 0.9155\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91650\n",
            "{'_Dataset_name': 'MNIST_1000samples', '_Model_setup': 'CNN', '_Model_id': '000011', 'Batch_size': 64, 'Optimizer': 'Adam_lr0.0001', 'Acivation': 'relu', 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
            "Model created.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 26, 26, 512)       5120      \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 346112)            0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                3461130   \n",
            "=================================================================\n",
            "Total params: 3,466,250\n",
            "Trainable params: 3,466,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 31s 514us/step - loss: 0.3068 - acc: 0.9186 - val_loss: 0.1375 - val_acc: 0.9619\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96190, saving model to model000011_epoch000001.hdf5\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 30s 502us/step - loss: 0.1122 - acc: 0.9698 - val_loss: 0.0846 - val_acc: 0.9768\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96190 to 0.97680, saving model to model000011_epoch000002.hdf5\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0759 - acc: 0.9792 - val_loss: 0.0744 - val_acc: 0.9775\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97680 to 0.97750, saving model to model000011_epoch000003.hdf5\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 0.0587 - acc: 0.9834 - val_loss: 0.0653 - val_acc: 0.9795\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.97750 to 0.97950, saving model to model000011_epoch000004.hdf5\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 30s 505us/step - loss: 0.0490 - acc: 0.9863 - val_loss: 0.0594 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.97950 to 0.98120, saving model to model000011_epoch000005.hdf5\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0417 - acc: 0.9883 - val_loss: 0.0552 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.98120 to 0.98250, saving model to model000011_epoch000006.hdf5\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0354 - acc: 0.9903 - val_loss: 0.0563 - val_acc: 0.9812\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.98250\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0313 - acc: 0.9910 - val_loss: 0.0566 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98250\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 30s 504us/step - loss: 0.0273 - acc: 0.9928 - val_loss: 0.0562 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98250\n",
            "Epoch 10/10\n",
            "21696/60000 [=========>....................] - ETA: 18s - loss: 0.0233 - acc: 0.9942Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2lQzq96NY1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Plot the results\n",
        "# pd.options.display.max_rows = 8\n",
        "lfp.plot_log(show_table=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8zre0uERDU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "https://keras.io/callbacks/\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
        "\n",
        "https://keras.io/getting-started/sequential-model-guide/"
      ]
    }
  ]
}