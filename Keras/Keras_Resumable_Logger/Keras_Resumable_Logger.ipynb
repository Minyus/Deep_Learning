{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Resumable_Logger.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vRgMOgH-ajJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up Google Colab environment"
      ]
    },
    {
      "metadata": {
        "id": "7F29nuHfiiUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Set colaboratory True to run in Google Colaboratory. \n",
        "colab = True\n",
        "\n",
        "## Mount Google Drive\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import os\n",
        "    ## Specify a directory in Google Drive\n",
        "    dir = '/content/drive/My Drive/Colab Notebooks/Keras_Resumable_Logger'\n",
        "    os.chdir(dir)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXFB9-C8lfDe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Check the environment info\n",
        "if colab:\n",
        "    os.getcwd()\n",
        "    #os.listdir()\n",
        "    print('## Check the uptime. (Google Colab reboots every 12 hours)')\n",
        "    !cat /proc/uptime | awk '{print \"Uptime is \" $1 /60 /60 \" hours (\" $1 \" sec)\"}'\n",
        "    print('## Check the GPU info')\n",
        "    !nvidia-smi\n",
        "    print('## Check the Python version') \n",
        "    !python --version\n",
        "    print('## Check the OS') \n",
        "    !cat /etc/issue\n",
        "    print('## Check the memory')\n",
        "    !free -h\n",
        "    print('## Check the disk')\n",
        "    !df -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QwFUcdCfa4Vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up resumable logging feature for Keras"
      ]
    },
    {
      "metadata": {
        "id": "E9G7gnSRShaG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print('TenslorFlow version: ', tf.__version__)\n",
        "\n",
        "class ResumableLogger():\n",
        "    def __init__(self, resume_if_possible = True, log_file_path = 'training_log.csv', verbose = 1):\n",
        "        self.log_file_path = log_file_path\n",
        "        self.verbose = verbose\n",
        "        self.update()\n",
        "        if not resume_if_possible:\n",
        "            self.clean_up()\n",
        "            self.update()\n",
        "    def update(self):\n",
        "        log_file_path = self.log_file_path\n",
        "        log_file_list = glob.glob(log_file_path)\n",
        "        log_file_list.sort()\n",
        "        self.log_file_list = log_file_list\n",
        "        if self.verbose >= 2:\n",
        "            print('Log file:', log_file_list)\n",
        "        log_file_exists = len(log_file_list) >= 1\n",
        "        self.log_file_exists = log_file_exists\n",
        "    def initial_model_id_num(self, epochs):\n",
        "        initial_model_id_num_ = \\\n",
        "        int(self.get_column_value('_Model_id')) + \\\n",
        "        int(int(self.get_column_value('epoch')) >= (epochs - 1))\n",
        "        if self.verbose >= 1:\n",
        "            print('Start training from model id:', initial_model_id_num_)\n",
        "        return initial_model_id_num_\n",
        "    def truncate(self, epoch_from_model_file):\n",
        "        log_file_path = self.log_file_path\n",
        "        self.update()\n",
        "        ## Remove rows after the epoch of the latest saved model \n",
        "        ## (needed because save_best_only option of checkpoint is set to True.)\n",
        "        epoch_from_log_file = int(self.get_column_value('epoch'))\n",
        "        epoch_diff = epoch_from_log_file - epoch_from_model_file \n",
        "        with open(log_file_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        if self.verbose >= 3:\n",
        "            print('Log file contents read:', lines)\n",
        "        lines_truncated = lines[:-(epoch_diff + 1)]\n",
        "        with open(log_file_path, 'w') as f:\n",
        "            f.writelines(lines_truncated)\n",
        "        if self.verbose >= 3:\n",
        "            print('New log file contents', lines_truncated)\n",
        "    def clean_up(self):\n",
        "        while self.log_file_exists:\n",
        "            os.remove(self.log_file_path)\n",
        "            self.update()\n",
        "            if self.verbose >= 1:\n",
        "                print('Log file deleted.')            \n",
        "        if False: ## Old code; to be deleted\n",
        "            self.update()\n",
        "            if self.log_file_exists:\n",
        "                for f in self.log_file_list:\n",
        "                    os.remove(f)\n",
        "                self.update()\n",
        "    def get_df(self, dtype=None):\n",
        "        self.update()\n",
        "        if self.log_file_exists:\n",
        "            try:\n",
        "                return pd.read_csv(self.log_file_path, dtype=dtype)\n",
        "            except:\n",
        "                pass\n",
        "        if self.verbose >= 2:\n",
        "            print('Log file was not read as a DataFrame.')\n",
        "        return None\n",
        "    def get_column_value(self, column, default = '0'):\n",
        "        log_df = self.get_df(dtype=str)\n",
        "        if log_df is not None:\n",
        "            if log_df.shape[0] >= 1:\n",
        "                column_value = log_df.iloc[-1][column]\n",
        "                return column_value\n",
        "        if self.verbose >= 2:\n",
        "            print('{} assumed to be {}.'.format(column, default))\n",
        "        column_value = default\n",
        "        return column_value   \n",
        "    ##\n",
        "    def plot_log(self, show_table = True):\n",
        "        log_all_df = self.get_df()\n",
        "        if log_all_df is not None:\n",
        "            model_id_list = log_all_df['_Model_id'].unique()\n",
        "            print('Model id list: ', model_id_list)\n",
        "            for model_id in model_id_list: \n",
        "                print('### Model id: ', model_id)\n",
        "                log_df = log_all_df.query('_Model_id == \"{}\"'.format(model_id))\n",
        "                if show_table:\n",
        "                    display(log_df)\n",
        "                log_df.set_index('epoch')[['acc', 'val_acc']].plot(style='.-', title='Accuracy vs Epoch')\n",
        "                plt.show()\n",
        "                log_df.set_index('epoch')[['loss', 'val_loss']].plot(style='.-', title='Loss vs Epoch')\n",
        "                plt.show()\n",
        "                log_df.set_index('epoch')[['Comp_time', 'Cum_comp_time']].plot(style='.-', title='Computation Time (sec) vs Epoch')\n",
        "                plt.show()   \n",
        "    def setup_model_and_callbacks(self, create_model, input_shape, resume_if_possible = True,\n",
        "                             model_param_dict = {}, verbose = 1, early_stopping_patience = None):   \n",
        "        mfp = ModelFileProcessing(model_id = model_param_dict['_Model_id'], verbose = 1)\n",
        "        resume_flag = resume_if_possible and mfp.model_file_exists and self.log_file_exists ## To Do: if the model Id entry exists in the CSV log file.\n",
        "\n",
        "        if resume_flag:\n",
        "            ## Load the saved model\n",
        "            model = keras.models.load_model(mfp.latest_model_file)\n",
        "            print('Use {} to resume fitting.'.format(mfp.latest_model_file))\n",
        "            initial_epoch = mfp.latest_epoch\n",
        "            ## Truncate the log file\n",
        "            self.truncate(epoch_from_model_file = mfp.latest_epoch) \n",
        "\n",
        "        if not resume_flag:\n",
        "            ## Create a basic model instance\n",
        "            model = create_model(input_shape)\n",
        "            print('Model created.')\n",
        "            initial_epoch = 0\n",
        "            ## Delete all model files if they exist.\n",
        "            mfp.clean_up_all()\n",
        "\n",
        "        ## Create checkpoint callback\n",
        "        check_point_ = tf.keras.callbacks.ModelCheckpoint(filepath = mfp.model_file_path, \n",
        "            monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'auto', \n",
        "            save_weights_only = False, period = 1)\n",
        "\n",
        "        ## Create old model file cleanup callback\n",
        "        old_model_file_cleanup_ = OldModelFileCleanup(mfp)\n",
        "\n",
        "        ## Create computation time callback\n",
        "        comp_time_ = CompTime(self)\n",
        "\n",
        "        ## Create model params callback\n",
        "        model_params_ = ModelParams(model_param_dict)\n",
        "\n",
        "        ## Create CSV logger callback\n",
        "        csv_logger_ = tf.keras.callbacks.CSVLogger(filename = self.log_file_path, separator=',',\n",
        "                                                   append = True)\n",
        "\n",
        "        callbacks = [model_params_, comp_time_, csv_logger_, \n",
        "                     check_point_, old_model_file_cleanup_ ]\n",
        "        \n",
        "        if early_stopping_patience is not None:        \n",
        "            ## Create early stopping callback\n",
        "            early_stopping_ = tf.keras.callbacks.EarlyStopping(monitor='val_acc', \n",
        "                min_delta=0, patience=early_stopping_patience, \n",
        "                verbose=1, mode='auto', baseline=None)\n",
        "            callbacks.append(early_stopping_)\n",
        "            \n",
        "        return model, callbacks, initial_epoch\n",
        "            \n",
        "class ModelFileProcessing():\n",
        "    def __init__(self, model_id = '', model_file_prefix = 'model', model_file_suffix = '.hdf5', verbose = 1):\n",
        "        model_file_prefix = model_file_prefix + model_id + '_epoch'\n",
        "        self.model_file_prefix = model_file_prefix\n",
        "        self.model_file_suffix = model_file_suffix\n",
        "        self.verbose = verbose\n",
        "        model_file_path = model_file_prefix + '{epoch:06d}' + model_file_suffix\n",
        "        self.model_file_path = model_file_path\n",
        "        self.update()\n",
        "    def update(self):\n",
        "        model_file_prefix = self.model_file_prefix\n",
        "        model_file_suffix = self.model_file_suffix\n",
        "        model_file_list = glob.glob(model_file_prefix + '*' + model_file_suffix)\n",
        "        model_file_list.sort()\n",
        "        if self.verbose >= 2:\n",
        "            print('Model files: ', model_file_list)\n",
        "        self.model_file_list = model_file_list\n",
        "        model_file_exists = len(model_file_list) >= 1\n",
        "        self.model_file_exists = model_file_exists\n",
        "\n",
        "        latest_model_file = model_file_list[-1] if model_file_exists else None\n",
        "        self.latest_model_file = latest_model_file\n",
        "\n",
        "        latest_epoch = int(latest_model_file[len(model_file_prefix):-len(model_file_suffix)]) \\\n",
        "            if model_file_exists else 0\n",
        "        self.latest_epoch = latest_epoch\n",
        "\n",
        "        multiple_model_files_exist = len(model_file_list) >= 2\n",
        "        self.multiple_model_files_exist = multiple_model_files_exist\n",
        "    def clean_up_old(self):\n",
        "        self.update()\n",
        "        ## Delete all model files excpet the latest to save space\n",
        "        if self.multiple_model_files_exist:\n",
        "            for f in self.model_file_list[:-1]:\n",
        "                os.remove(f)  \n",
        "    def clean_up_all(self):\n",
        "        self.update()\n",
        "        ## Delete all model files if they exist.\n",
        "        if self.model_file_exists:\n",
        "            for f in self.model_file_list:\n",
        "                os.remove(f)   \n",
        "                \n",
        "class ModelParams(keras.callbacks.Callback):\n",
        "    def __init__(self, model_param_dict = {}, verbose = 2):\n",
        "        self.model_param_dict = model_param_dict\n",
        "        self.verbose = verbose\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if self.verbose >= 3:\n",
        "            print('Model params: ', self.model_param_dict)\n",
        "        ## Add the model parameters to the logs\n",
        "        logs.update(self.model_param_dict)\n",
        "        if self.verbose >= 3:\n",
        "            print('logs: ', logs)\n",
        "        \n",
        "class OldModelFileCleanup(keras.callbacks.Callback):\n",
        "    def __init__(self, mfp):\n",
        "        self.mfp = mfp\n",
        "    def on_epoch_end(self, epoch, logs={}):     \n",
        "        self.mfp.clean_up_old()\n",
        "        \n",
        "import time          \n",
        "class CompTime(keras.callbacks.Callback):\n",
        "    def __init__(self, rl):\n",
        "        self.rl = rl\n",
        "        self.verbose = rl.verbose\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        ## Calculate computation time for the epoch\n",
        "        comp_time = time.time() - self.epoch_time_start\n",
        "        logs['Comp_time'] = comp_time\n",
        "        if self.verbose >= 2:\n",
        "            print('Computation time: {} sec'.format(comp_time))\n",
        "        ## Calculate cumulative computation time up to the epoch\n",
        "        cum_comp_time_past = \\\n",
        "        float(self.rl.get_column_value('Cum_comp_time')) if epoch >= 1 else 0\n",
        "        logs['Cum_comp_time'] = cum_comp_time_past + comp_time\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWwcu32T3aAp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up dataset"
      ]
    },
    {
      "metadata": {
        "id": "hp7CMqwQ3Ymp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dataset_name = 'MNIST'\n",
        "dataset_name = 'MNIST_1000samples'\n",
        "\n",
        "if dataset_name in ['MNIST', 'MNIST_1000samples']:\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    print('x_train: ', x_train.shape)\n",
        "    print('y_train', y_train.shape)\n",
        "    print('x_test: ', x_test.shape)\n",
        "    print('y_test', y_test.shape)\n",
        "\n",
        "    input_shape = x_train.shape[1:]\n",
        "    print('input_shape: ', input_shape )\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "    \n",
        "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "\n",
        "if dataset_name == 'MNIST_1000samples':\n",
        "    ## Reduce samples size to speed up \n",
        "    n = 1000\n",
        "    y_train = y_train[:n]\n",
        "    y_test = y_test[:n]\n",
        "    x_train = x_train[:n]\n",
        "    x_test = x_test[:n]\n",
        "    \n",
        "print('\\n### data after processing')\n",
        "print('x_train: ', x_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('x_test: ', x_test.shape)\n",
        "print('y_test', y_test.shape)\n",
        "\n",
        "input_shape = x_train.shape[1:]\n",
        "print('input_shape: ', input_shape )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btAFN4H54FOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up model parameters"
      ]
    },
    {
      "metadata": {
        "id": "kgsQhkdy4DxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NotUsed=\"\"\"\n",
        "### Grid Search \n",
        " \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout\n",
        "from keras.optimizers import SGD, Adagrad, RMSprop, Adam\n",
        "\n",
        "batch_size_list = [64]\n",
        "\n",
        "optimizer_dict = {}\n",
        "#optimizer_dict['SGD'] = SGD()\n",
        "#optimizer_dict['SGD_momentum_0.5'] = SGD(momentum = 0.5)\n",
        "#optimizer_dict['SGD_momentum_0.9'] = SGD(momentum = 0.9)\n",
        "#optimizer_dict['SGD_momentum_0.99'] = SGD(momentum = 0.99)         \n",
        "#optimizer_dict['Adagrad'] = Adagrad()\n",
        "#optimizer_dict['RMSprop'] = RMSprop()\n",
        "optimizer_dict['Adam'] = Adam()\n",
        "\n",
        "activation_list = []\n",
        "activation_list.append('relu')\n",
        "#activation_list.append('sigmoid')\n",
        "#activation_list.append('elu')\n",
        "\n",
        "objective_list = []\n",
        "objective_list.append('sparse_categorical_crossentropy')\n",
        "#objective_list.append('categorical_crossentropy')\n",
        "\n",
        "dropout_rate_list = [0.0, 0.5]\n",
        "\n",
        "model_param_dict_list = []\n",
        "i = 0\n",
        "for batch_size in batch_size_list:\n",
        "    for optimizer, _ in optimizer_dict.items():\n",
        "        for activation in activation_list:\n",
        "            for objective in objective_list:\n",
        "                for dropout_rate in dropout_rate_list:\n",
        "                    d = {}\n",
        "                    d['_Dataset_name'] = dataset_name\n",
        "                    d['_Model_setup'] = 'Dense_Only'\n",
        "                    d['_Model_id'] = '{:06d}'.format(i)\n",
        "                    d['Batch_size'] = batch_size\n",
        "                    d['Optimizer'] = optimizer\n",
        "                    d['Acivation'] = activation\n",
        "                    d['Objective'] = objective\n",
        "                    d['Dropout_rate'] = dropout_rate\n",
        "                    i += 1\n",
        "                    print(d)\n",
        "                    model_param_dict_list.append(d)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZqBQ5UsCF12F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Create model_param_dict_list (derivative style)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D\n",
        "from keras.optimizers import SGD, Adagrad, RMSprop, Adam, Adadelta, Adamax, Nadam\n",
        "\n",
        "optimizer_dict = {\\\n",
        "'SGD': SGD(),\n",
        "'SGD_momentum_0.5': SGD(momentum = 0.5),\n",
        "'SGD_momentum_0.9': SGD(momentum = 0.9),\n",
        "'SGD_momentum_0.99': SGD(momentum = 0.99),\n",
        "'SGD_momentum_0.999': SGD(momentum = 0.999),\n",
        "'SGD_momentum_0.9999': SGD(momentum = 0.9999),\n",
        "'SGD_nesterov_momentum_0.9': SGD(momentum = 0.9, nesterov=True),\n",
        "'SGD_nesterov_momentum_0.9_decay_1e-6': SGD(momentum = 0.9, decay=1e-6, nesterov=True),\n",
        "'SGD_momentum_0.9_decay_1e-6': SGD(momentum = 0.9, decay=1e-6),\n",
        "'Adagrad': Adagrad(),\n",
        "'RMSprop': RMSprop(),\n",
        "'Adadelta': Adadelta(),\n",
        "'Adamax': Adamax(),\n",
        "'Adadelta': Adadelta(),\n",
        "'Nadam': Nadam(),\n",
        "'Adam': Adam(lr=0.001),\n",
        "'Adam_lr0.0001': Adam(lr=0.0001),\n",
        "'Adam_lr0.01': Adam(lr=0.01),\n",
        "'Adam_lr0.1': Adam(lr=0.1),\n",
        "'Adam_lr1e-5': Adam(lr=1e-5) }\n",
        "\n",
        "base_model_param_dict = \\\n",
        "{'_Dataset_name': dataset_name, '_Model_setup': 'CNN', '_Model_id': '000000', \n",
        " 'Batch_size': 64, 'Optimizer': 'Adam', 'Acivation': 'relu', \n",
        " 'Objective': 'sparse_categorical_crossentropy', 'Dropout_rate': 0.0}\n",
        "derivative_dict_list = [\\\n",
        " {'Dropout_rate': 0.3},\n",
        " {'Dropout_rate': 0.5},\n",
        " {'Dropout_rate': 0.7},\n",
        " {'Dropout_rate': 0.9},\n",
        " {'Acivation': 'sigmoid'}, \n",
        " {'Acivation': 'elu'},\n",
        " {'Acivation': 'tanh'},\n",
        " {'Acivation': 'selu'},\n",
        " {'Acivation': 'hard_sigmoid'},\n",
        " {'Acivation': 'linear'},\n",
        " {'Optimizer': 'Adam_lr0.0001'},\n",
        " {'Optimizer': 'Adam_lr0.01'},\n",
        " {'Optimizer': 'Adam_lr0.1'},\n",
        " {'Optimizer': 'SGD'},\n",
        " {'Optimizer': 'SGD_momentum_0.5'},\n",
        " {'Optimizer': 'SGD_momentum_0.9'},\n",
        " {'Optimizer': 'SGD_momentum_0.99'},\n",
        " {'Optimizer': 'RMSprop'},\n",
        " {'Optimizer': 'Adagrad'},\n",
        " {'Optimizer': 'Adadelta'},\n",
        " {'Batch_size': 32},\n",
        " {'Batch_size': 128},\n",
        " {'Batch_size': 256},\n",
        " {'Optimizer': 'Nadam'},                        \n",
        " {'Optimizer': 'SGD_nesterov_momentum_0.9'},                       \n",
        " {'Optimizer': 'SGD_nesterov_momentum_0.9_decay_1e-6'},\n",
        " {'Optimizer': 'SGD_momentum_0.999'},\n",
        " {'Optimizer': 'SGD_momentum_0.9999'},\n",
        " {'Optimizer': 'SGD_momentum_0.9_decay_1e-6'}, \n",
        " {'Optimizer': 'Adam_lr1e-5'},\n",
        " {'Dropout_rate': 0.1}, \n",
        " {'Dropout_rate': 0.2}, \n",
        " {'Dropout_rate': 0.4},\n",
        " {'Dropout_rate': 0.6},\n",
        " {'Dropout_rate': 0.8},\n",
        " {'Batch_size': 512},\n",
        " {'Batch_size': 1024}]\n",
        "\n",
        "model_param_dict_list = [base_model_param_dict]\n",
        "print(base_model_param_dict)\n",
        "\n",
        "for i, derivative_dict in enumerate(derivative_dict_list, 1):\n",
        "    model_param_dict = base_model_param_dict.copy()\n",
        "    model_param_dict.update({'_Model_id': '{:06d}'.format(i)})\n",
        "    model_param_dict.update(derivative_dict)\n",
        "    model_param_dict_list.append(model_param_dict)\n",
        "    print(model_param_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-0RAJHl4i5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train models"
      ]
    },
    {
      "metadata": {
        "id": "jrXvLv4kjx-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Set resume_if_possible = True to resume using a model file if available.\n",
        "## Set resume_if_possible = False to force to start training from scratch.\n",
        "resume_if_possible = True\n",
        "#resume_if_possible = False\n",
        "\n",
        "if resume_if_possible:\n",
        "    print('Resume using a model file if available.')\n",
        "if not resume_if_possible:\n",
        "    print('Force to start training from scratch.')\n",
        "\n",
        "## Wait for 5 secs to give time for you to stop running. Ok to delete this.\n",
        "import time\n",
        "time.sleep(5) \n",
        "\n",
        "epochs = 10\n",
        "\n",
        "rl = ResumableLogger(resume_if_possible = resume_if_possible, verbose = 1)\n",
        "\n",
        "for model_param_dict in model_param_dict_list[rl.initial_model_id_num(epochs):]:\n",
        "    print(model_param_dict)\n",
        "    ## Assign variables as in the model_param_dict dictionary\n",
        "    globals().update(model_param_dict) \n",
        "    \n",
        "    if _Model_setup == 'Dense_Only':\n",
        "        def create_model(input_shape):\n",
        "            model = Sequential()\n",
        "            model.add(Flatten(input_shape = input_shape))\n",
        "            model.add(Dense(512, activation = Acivation))\n",
        "            model.add(Dropout(Dropout_rate))\n",
        "            model.add(Dense(10, activation='softmax'))\n",
        "            model.compile(optimizer = optimizer_dict[Optimizer], \n",
        "                          loss=Objective, metrics=['accuracy'])\n",
        "            return model\n",
        "    if _Model_setup == 'CNN': \n",
        "        def create_model(input_shape):\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(512, kernel_size = (3, 3), strides = (1, 1),\n",
        "                           activation = Acivation, input_shape = input_shape))\n",
        "            model.add(Flatten())\n",
        "            model.add(Dropout(Dropout_rate))\n",
        "            model.add(Dense(10, activation='softmax'))\n",
        "            model.compile(optimizer = optimizer_dict[Optimizer], \n",
        "                        loss=Objective, metrics=['accuracy'])\n",
        "            return model\n",
        "  \n",
        "    model, callbacks, initial_epoch = \\\n",
        "    rl.setup_model_and_callbacks(create_model, input_shape, \n",
        "        resume_if_possible = resume_if_possible, model_param_dict = model_param_dict, \n",
        "        verbose = 1, early_stopping_patience = None)\n",
        "\n",
        "    model.summary()\n",
        "    ## Fit \n",
        "    model.fit(x_train, y_train, validation_data = (x_test, y_test), shuffle = True,\n",
        "              batch_size = Batch_size, epochs = epochs, callbacks = callbacks, \n",
        "              initial_epoch = initial_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2lQzq96NY1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Plot the results\n",
        "# pd.options.display.max_rows = 8\n",
        "rl.plot_log(show_table=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8zre0uERDU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "https://keras.io/callbacks/\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
        "\n",
        "https://keras.io/getting-started/sequential-model-guide/"
      ]
    }
  ]
}